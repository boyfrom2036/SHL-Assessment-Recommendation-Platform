# -*- coding: utf-8 -*-
"""SHL Assigment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JJvVLaWLfYGkkIn_hECNnV2R83b58-P4
"""

!pip install requests beautifulsoup4

import requests
from bs4 import BeautifulSoup
import time

BASE_URL = "https://www.shl.com"
CATALOG_URL = BASE_URL + "/solutions/products/product-catalog/"
STEP = 12
MAX_START = 372  # Last value for 'start' in the URL (inclusive)

all_urls = set()

for start in range(0, MAX_START + 1, STEP):
    if start == 0:
        page_url = f"{CATALOG_URL}?type=1&type=1"
    else:
        page_url = f"{CATALOG_URL}?start={start}&type=1&type=1"
    print(f"Scraping: {page_url}")
    response = requests.get(page_url)
    if response.status_code != 200:
        print(f"Failed to retrieve page: {page_url}")
        continue
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the table wrapper for Individual Test Solutions
    wrappers = soup.find_all('div', class_='custom__table-wrapper')
    if not wrappers:
        print("No custom__table-wrapper found on this page.")
        continue
    # On first page, Individual Test Solutions is the second wrapper; others, it's the only one
    if start == 0 and len(wrappers) > 1:
        table_wrapper = wrappers[1]
    else:
        table_wrapper = wrappers[0]
    table = table_wrapper.find('table')
    if not table:
        print("No table found in the wrapper.")
        continue

    # Extract URLs from <td class="custom__table-heading__title">
    for td in table.find_all('td', class_='custom__table-heading__title'):
        link_tag = td.find('a', href=True)
        if link_tag:
            href = link_tag['href']
            if href.startswith('/'):
                href = BASE_URL + href
            all_urls.add(href)
    time.sleep(1)  # Be polite to the server

print(f"\nTotal unique individual test solution URLs found: {len(all_urls)}")
for url in sorted(all_urls):
    print(url)

len(all_urls)

import requests
from bs4 import BeautifulSoup
import re
import json
import time

code_map = {
    'A': 'Ability & Aptitude',
    'B': 'Biodata & Situational Judgement',
    'C': 'Competencies',
    'D': 'Development & 360',
    'E': 'Assessment Exercises',
    'K': 'Knowledge & Skills',
    'P': 'Personality & Behaviour',
    'S': 'Simulations'
}

def extract_support(soup, label):
    section = soup.find(string=re.compile(label, re.I))
    if section:
        parent = section.parent
        if 'ðŸŸ¢' in parent.get_text():
            return "Yes"
        elif parent.find('span', style=lambda s: s and 'green' in s):
            return "Yes"
        elif parent.find('svg', attrs={'fill': re.compile('green', re.I)}):
            return "Yes"
        else:
            return "Yes"
    return "No"

def extract_assessment_data(url):
    print(f"Scraping: {url}")
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    # --- Description ---
    description = ""
    desc_div = soup.find('div', class_="product-catalogue-training-calendar__row typ")
    if desc_div:
        h4 = desc_div.find('h4')
        if h4 and "description" in h4.text.lower():
            desc_p = h4.find_next('p')
            if desc_p:
                description = desc_p.get_text(strip=True)
    if not description:
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '').strip()

    # --- Job Levels ---
    job_levels = []
    job_level_header = soup.find(lambda tag: tag.name in ['h4', 'h3', 'h2'] and re.search(r"job\s*levels", tag.get_text(), re.I))
    if job_level_header:
        job_p = job_level_header.find_next('p')
        if job_p:
            job_levels = [j.strip() for j in job_p.get_text().split(',') if j.strip()]

    # --- Languages ---
    languages = []
    lang_header = soup.find(lambda tag: tag.name == 'h4' and "languages" in tag.get_text().lower())
    if lang_header:
        lang_p = lang_header.find_next('p')
        if lang_p:
            languages = [l.strip() for l in lang_p.get_text().split(',') if l.strip()]

    # --- Remote and Adaptive Support ---
    remote_support = extract_support(soup, "Remote Testing")
    adaptive_support = extract_support(soup, "Adaptive")

    # --- Duration ---
    duration = 0
    duration_pattern = re.compile(r"Approximate Completion Time in minutes\s*=\s*(\d{1,3})", re.I)
    duration_match = duration_pattern.search(soup.get_text())
    if duration_match:
        duration = int(duration_match.group(1))
    else:
        duration_candidates = soup.find_all(string=re.compile(r"\d+\s*(min|minute)", re.I))
        for cand in duration_candidates:
            match = re.search(r"(\d{1,3})\s*(min|minute)", cand, re.I)
            if match and 0 < int(match.group(1)) < 300:
                duration = int(match.group(1))
                break

    # --- Test Type ---
    test_type = []
    # Find the <span class="d-flex ms-2"> container
    badge_container = soup.find('span', class_='d-flex ms-2')
    if badge_container:
        for badge in badge_container.find_all(
            'span',
            class_='product-catalogue__key',
            attrs={'data-tooltip': 'productCatalogueTooltip', 'data-has-tooltip': 'true'}
        ):
            letter = badge.get_text(strip=True)
            if letter in code_map and code_map[letter] not in test_type:
                test_type.append(code_map[letter])
    else:
        # Fallback: look for badges anywhere with the tooltip attribute and correct class
        for badge in soup.find_all(
            'span',
            class_='product-catalogue__key',
            attrs={'data-tooltip': 'productCatalogueTooltip', 'data-has-tooltip': 'true'}
        ):
            letter = badge.get_text(strip=True)
            if letter in code_map and code_map[letter] not in test_type:
                test_type.append(code_map[letter])

    return {
        "url": url,
        "adaptive_support": adaptive_support,
        "description": description,
        "duration": duration,
        "remote_support": remote_support,
        "test_type": test_type,
        "job_levels": job_levels,
        "languages": languages
    }

# all_urls should be your list of 377 URLs
results = []
for idx, url in enumerate(all_urls, 1):
    try:
        data = extract_assessment_data(url)
        results.append(data)
        print(f"Scraped {idx} of {len(all_urls)} URLs\n")
        time.sleep(0.5)  # Be polite to the server
        # Print results only once after the 3rd scraping
        if idx == 3:
            print(f"\nResults after 3 scrapings:\n")
            print(json.dumps({"recommended_assessments": results[:3]}, indent=2))
    except Exception as e:
        print(f"Error scraping {url}: {e}")

# Save all results at the end
with open("shl_recommended_assessments.json", "w") as f:
    json.dump({"recommended_assessments": results}, f, indent=2)